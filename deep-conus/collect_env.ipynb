{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e25fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "PyTorch version: 1.12.1\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 11.3\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Microsoft Windows 11 Home\n",
      "GCC version: Could not collect\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "Libc version: N/A\n",
      "\n",
      "Python version: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)] (64-bit runtime)\n",
      "Python platform: Windows-10-10.0.22621-SP0\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "CUDA_MODULE_LOADING set to: \n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Nvidia driver version: 522.06\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] mypy-extensions==0.4.3\n",
      "[pip3] numpy==1.21.5\n",
      "[pip3] numpydoc==1.2\n",
      "[pip3] torch==1.12.1\n",
      "[pip3] torchaudio==0.12.1\n",
      "[pip3] torchvision==0.13.1\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] cudatoolkit               11.3.1               h59b6b97_2  \n",
      "[conda] mkl                       2021.4.0           haa95532_640  \n",
      "[conda] mkl-service               2.4.0            py39h2bbff1b_0  \n",
      "[conda] mkl_fft                   1.3.1            py39h277e83a_0  \n",
      "[conda] mkl_random                1.2.2            py39hf11a4ad_0  \n",
      "[conda] numpy                     1.21.5           py39h7a0a035_1  \n",
      "[conda] numpy-base                1.21.5           py39hca35cd5_1  \n",
      "[conda] numpydoc                  1.2                pyhd3eb1b0_0  \n",
      "[conda] pytorch                   1.12.1          py3.9_cuda11.3_cudnn8_0    pytorch\n",
      "[conda] pytorch-mutex             1.0                        cuda    pytorch\n",
      "[conda] torch                     1.12.1                   pypi_0    pypi\n",
      "[conda] torchaudio                0.12.1               py39_cu113    pytorch\n",
      "[conda] torchvision               0.13.1                   pypi_0    pypi\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Unlike the rest of the PyTorch this file must be python2 compliant.\n",
    "# This script outputs relevant system environment info\n",
    "# Run it with `python collect_env.py`.\n",
    "import datetime\n",
    "import locale\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except (ImportError, NameError, AttributeError, OSError):\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# System Environment Information\n",
    "SystemEnv = namedtuple('SystemEnv', [\n",
    "    'torch_version',\n",
    "    'is_debug_build',\n",
    "    'cuda_compiled_version',\n",
    "    'gcc_version',\n",
    "    'clang_version',\n",
    "    'cmake_version',\n",
    "    'os',\n",
    "    'libc_version',\n",
    "    'python_version',\n",
    "    'python_platform',\n",
    "    'is_cuda_available',\n",
    "    'cuda_runtime_version',\n",
    "    'cuda_module_loading',\n",
    "    'nvidia_driver_version',\n",
    "    'nvidia_gpu_models',\n",
    "    'cudnn_version',\n",
    "    'pip_version',  # 'pip' or 'pip3'\n",
    "    'pip_packages',\n",
    "    'conda_packages',\n",
    "    'hip_compiled_version',\n",
    "    'hip_runtime_version',\n",
    "    'miopen_runtime_version',\n",
    "    'caching_allocator_config',\n",
    "    'is_xnnpack_available',\n",
    "])\n",
    "\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE, shell=True)\n",
    "    raw_output, raw_err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    if get_platform() == 'win32':\n",
    "        enc = 'oem'\n",
    "    else:\n",
    "        enc = locale.getpreferredencoding()\n",
    "    output = raw_output.decode(enc)\n",
    "    err = raw_err.decode(enc)\n",
    "    return rc, output.strip(), err.strip()\n",
    "\n",
    "\n",
    "def run_and_read_all(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda; reads and returns entire output if rc is 0\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_and_parse_first_match(run_lambda, command, regex):\n",
    "    \"\"\"Runs command using run_lambda, returns the first regex match if it exists\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    match = re.search(regex, out)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "def run_and_return_first_line(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda and returns first line if output is not empty\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    return out.split('\\n')[0]\n",
    "\n",
    "\n",
    "def get_conda_packages(run_lambda):\n",
    "    conda = os.environ.get('CONDA_EXE', 'conda')\n",
    "    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n",
    "    if out is None:\n",
    "        return out\n",
    "\n",
    "    return \"\\n\".join(\n",
    "        line\n",
    "        for line in out.splitlines()\n",
    "        if not line.startswith(\"#\")\n",
    "        and any(\n",
    "            name in line\n",
    "            for name in {\n",
    "                \"torch\",\n",
    "                \"numpy\",\n",
    "                \"cudatoolkit\",\n",
    "                \"soumith\",\n",
    "                \"mkl\",\n",
    "                \"magma\",\n",
    "                \"mkl\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_gcc_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')\n",
    "\n",
    "def get_clang_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'clang --version', r'clang version (.*)')\n",
    "\n",
    "\n",
    "def get_cmake_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cmake --version', r'cmake (.*)')\n",
    "\n",
    "\n",
    "def get_nvidia_driver_version(run_lambda):\n",
    "    if get_platform() == 'darwin':\n",
    "        cmd = 'kextstat | grep -i cuda'\n",
    "        return run_and_parse_first_match(run_lambda, cmd,\n",
    "                                         r'com[.]nvidia[.]CUDA [(](.*?)[)]')\n",
    "    smi = get_nvidia_smi()\n",
    "    return run_and_parse_first_match(run_lambda, smi, r'Driver Version: (.*?) ')\n",
    "\n",
    "\n",
    "def get_gpu_info(run_lambda):\n",
    "    if get_platform() == 'darwin' or (TORCH_AVAILABLE and hasattr(torch.version, 'hip') and torch.version.hip is not None):\n",
    "        if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "            return torch.cuda.get_device_name(None)\n",
    "        return None\n",
    "    smi = get_nvidia_smi()\n",
    "    uuid_regex = re.compile(r' \\(UUID: .+?\\)')\n",
    "    rc, out, _ = run_lambda(smi + ' -L')\n",
    "    if rc != 0:\n",
    "        return None\n",
    "    # Anonymize GPUs by removing their UUID\n",
    "    return re.sub(uuid_regex, '', out)\n",
    "\n",
    "\n",
    "def get_running_cuda_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'nvcc --version', r'release .+ V(.*)')\n",
    "\n",
    "\n",
    "def get_cudnn_version(run_lambda):\n",
    "    \"\"\"This will return a list of libcudnn.so; it's hard to tell which one is being used\"\"\"\n",
    "    if get_platform() == 'win32':\n",
    "        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n",
    "        cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n",
    "        where_cmd = os.path.join(system_root, 'System32', 'where')\n",
    "        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n",
    "    elif get_platform() == 'darwin':\n",
    "        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n",
    "        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n",
    "        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n",
    "        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n",
    "        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'\n",
    "    else:\n",
    "        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d\" \" -f1 | rev'\n",
    "    rc, out, _ = run_lambda(cudnn_cmd)\n",
    "    # find will return 1 if there are permission errors or if not found\n",
    "    if len(out) == 0 or (rc != 1 and rc != 0):\n",
    "        l = os.environ.get('CUDNN_LIBRARY')\n",
    "        if l is not None and os.path.isfile(l):\n",
    "            return os.path.realpath(l)\n",
    "        return None\n",
    "    files_set = set()\n",
    "    for fn in out.split('\\n'):\n",
    "        fn = os.path.realpath(fn)  # eliminate symbolic links\n",
    "        if os.path.isfile(fn):\n",
    "            files_set.add(fn)\n",
    "    if not files_set:\n",
    "        return None\n",
    "    # Alphabetize the result because the order is non-deterministic otherwise\n",
    "    files = list(sorted(files_set))\n",
    "    if len(files) == 1:\n",
    "        return files[0]\n",
    "    result = '\\n'.join(files)\n",
    "    return 'Probably one of the following:\\n{}'.format(result)\n",
    "\n",
    "\n",
    "def get_nvidia_smi():\n",
    "    # Note: nvidia-smi is currently available only on Windows and Linux\n",
    "    smi = 'nvidia-smi'\n",
    "    if get_platform() == 'win32':\n",
    "        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n",
    "        program_files_root = os.environ.get('PROGRAMFILES', 'C:\\\\Program Files')\n",
    "        legacy_path = os.path.join(program_files_root, 'NVIDIA Corporation', 'NVSMI', smi)\n",
    "        new_path = os.path.join(system_root, 'System32', smi)\n",
    "        smis = [new_path, legacy_path]\n",
    "        for candidate_smi in smis:\n",
    "            if os.path.exists(candidate_smi):\n",
    "                smi = '\"{}\"'.format(candidate_smi)\n",
    "                break\n",
    "    return smi\n",
    "\n",
    "\n",
    "def get_platform():\n",
    "    if sys.platform.startswith('linux'):\n",
    "        return 'linux'\n",
    "    elif sys.platform.startswith('win32'):\n",
    "        return 'win32'\n",
    "    elif sys.platform.startswith('cygwin'):\n",
    "        return 'cygwin'\n",
    "    elif sys.platform.startswith('darwin'):\n",
    "        return 'darwin'\n",
    "    else:\n",
    "        return sys.platform\n",
    "\n",
    "\n",
    "def get_mac_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion', r'(.*)')\n",
    "\n",
    "\n",
    "def get_windows_version(run_lambda):\n",
    "    system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n",
    "    wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n",
    "    findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n",
    "    return run_and_read_all(run_lambda, '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n",
    "\n",
    "\n",
    "def get_lsb_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'lsb_release -a', r'Description:\\t(.*)')\n",
    "\n",
    "\n",
    "def check_release_file(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',\n",
    "                                     r'PRETTY_NAME=\"(.*)\"')\n",
    "\n",
    "\n",
    "def get_os(run_lambda):\n",
    "    from platform import machine\n",
    "    platform = get_platform()\n",
    "\n",
    "    if platform == 'win32' or platform == 'cygwin':\n",
    "        return get_windows_version(run_lambda)\n",
    "\n",
    "    if platform == 'darwin':\n",
    "        version = get_mac_version(run_lambda)\n",
    "        if version is None:\n",
    "            return None\n",
    "        return 'macOS {} ({})'.format(version, machine())\n",
    "\n",
    "    if platform == 'linux':\n",
    "        # Ubuntu/Debian based\n",
    "        desc = get_lsb_version(run_lambda)\n",
    "        if desc is not None:\n",
    "            return '{} ({})'.format(desc, machine())\n",
    "\n",
    "        # Try reading /etc/*-release\n",
    "        desc = check_release_file(run_lambda)\n",
    "        if desc is not None:\n",
    "            return '{} ({})'.format(desc, machine())\n",
    "\n",
    "        return '{} ({})'.format(platform, machine())\n",
    "\n",
    "    # Unknown platform\n",
    "    return platform\n",
    "\n",
    "\n",
    "def get_python_platform():\n",
    "    import platform\n",
    "    return platform.platform()\n",
    "\n",
    "\n",
    "def get_libc_version():\n",
    "    import platform\n",
    "    if get_platform() != 'linux':\n",
    "        return 'N/A'\n",
    "    return '-'.join(platform.libc_ver())\n",
    "\n",
    "\n",
    "def get_pip_packages(run_lambda):\n",
    "    \"\"\"Returns `pip list` output. Note: will also find conda-installed pytorch\n",
    "    and numpy packages.\"\"\"\n",
    "    # People generally have `pip` as `pip` or `pip3`\n",
    "    # But here it is incoved as `python -mpip`\n",
    "    def run_with_pip(pip):\n",
    "        out = run_and_read_all(run_lambda, \"{} list --format=freeze\".format(pip))\n",
    "        return \"\\n\".join(\n",
    "            line\n",
    "            for line in out.splitlines()\n",
    "            if any(\n",
    "                name in line\n",
    "                for name in {\n",
    "                    \"torch\",\n",
    "                    \"numpy\",\n",
    "                    \"mypy\",\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    pip_version = 'pip3' if sys.version[0] == '3' else 'pip'\n",
    "    out = run_with_pip(sys.executable + ' -mpip')\n",
    "\n",
    "    return pip_version, out\n",
    "\n",
    "\n",
    "def get_cachingallocator_config():\n",
    "    ca_config = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')\n",
    "    return ca_config\n",
    "\n",
    "\n",
    "def get_cuda_module_loading_config():\n",
    "    if TORCH_AVAILABLE and torch.cuda.is_available():\n",
    "        torch.cuda.init()\n",
    "        config = os.environ.get('CUDA_MODULE_LOADING', '')\n",
    "        return config\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "\n",
    "def is_xnnpack_available():\n",
    "    if TORCH_AVAILABLE:\n",
    "        import torch.backends.xnnpack\n",
    "        return str(torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_env_info():\n",
    "    run_lambda = run\n",
    "    pip_version, pip_list_output = get_pip_packages(run_lambda)\n",
    "\n",
    "    if TORCH_AVAILABLE:\n",
    "        version_str = torch.__version__\n",
    "        debug_mode_str = str(torch.version.debug)\n",
    "        cuda_available_str = str(torch.cuda.is_available())\n",
    "        cuda_version_str = torch.version.cuda\n",
    "        if not hasattr(torch.version, 'hip') or torch.version.hip is None:  # cuda version\n",
    "            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n",
    "        else:  # HIP version\n",
    "            cfg = torch._C._show_config().split('\\n')\n",
    "            hip_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'HIP Runtime' in s][0]\n",
    "            miopen_runtime_version = [s.rsplit(None, 1)[-1] for s in cfg if 'MIOpen' in s][0]\n",
    "            cuda_version_str = 'N/A'\n",
    "            hip_compiled_version = torch.version.hip\n",
    "    else:\n",
    "        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\n",
    "        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n",
    "\n",
    "    sys_version = sys.version.replace(\"\\n\", \" \")\n",
    "\n",
    "    return SystemEnv(\n",
    "        torch_version=version_str,\n",
    "        is_debug_build=debug_mode_str,\n",
    "        python_version='{} ({}-bit runtime)'.format(sys_version, sys.maxsize.bit_length() + 1),\n",
    "        python_platform=get_python_platform(),\n",
    "        is_cuda_available=cuda_available_str,\n",
    "        cuda_compiled_version=cuda_version_str,\n",
    "        cuda_runtime_version=get_running_cuda_version(run_lambda),\n",
    "        cuda_module_loading=get_cuda_module_loading_config(),\n",
    "        nvidia_gpu_models=get_gpu_info(run_lambda),\n",
    "        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n",
    "        cudnn_version=get_cudnn_version(run_lambda),\n",
    "        hip_compiled_version=hip_compiled_version,\n",
    "        hip_runtime_version=hip_runtime_version,\n",
    "        miopen_runtime_version=miopen_runtime_version,\n",
    "        pip_version=pip_version,\n",
    "        pip_packages=pip_list_output,\n",
    "        conda_packages=get_conda_packages(run_lambda),\n",
    "        os=get_os(run_lambda),\n",
    "        libc_version=get_libc_version(),\n",
    "        gcc_version=get_gcc_version(run_lambda),\n",
    "        clang_version=get_clang_version(run_lambda),\n",
    "        cmake_version=get_cmake_version(run_lambda),\n",
    "        caching_allocator_config=get_cachingallocator_config(),\n",
    "        is_xnnpack_available=is_xnnpack_available(),\n",
    "    )\n",
    "\n",
    "env_info_fmt = \"\"\"\n",
    "PyTorch version: {torch_version}\n",
    "Is debug build: {is_debug_build}\n",
    "CUDA used to build PyTorch: {cuda_compiled_version}\n",
    "ROCM used to build PyTorch: {hip_compiled_version}\n",
    "\n",
    "OS: {os}\n",
    "GCC version: {gcc_version}\n",
    "Clang version: {clang_version}\n",
    "CMake version: {cmake_version}\n",
    "Libc version: {libc_version}\n",
    "\n",
    "Python version: {python_version}\n",
    "Python platform: {python_platform}\n",
    "Is CUDA available: {is_cuda_available}\n",
    "CUDA runtime version: {cuda_runtime_version}\n",
    "CUDA_MODULE_LOADING set to: {cuda_module_loading}\n",
    "GPU models and configuration: {nvidia_gpu_models}\n",
    "Nvidia driver version: {nvidia_driver_version}\n",
    "cuDNN version: {cudnn_version}\n",
    "HIP runtime version: {hip_runtime_version}\n",
    "MIOpen runtime version: {miopen_runtime_version}\n",
    "Is XNNPACK available: {is_xnnpack_available}\n",
    "\n",
    "Versions of relevant libraries:\n",
    "{pip_packages}\n",
    "{conda_packages}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def pretty_str(envinfo):\n",
    "    def replace_nones(dct, replacement='Could not collect'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is not None:\n",
    "                continue\n",
    "            dct[key] = replacement\n",
    "        return dct\n",
    "\n",
    "    def replace_bools(dct, true='Yes', false='No'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is True:\n",
    "                dct[key] = true\n",
    "            elif dct[key] is False:\n",
    "                dct[key] = false\n",
    "        return dct\n",
    "\n",
    "    def prepend(text, tag='[prepend]'):\n",
    "        lines = text.split('\\n')\n",
    "        updated_lines = [tag + line for line in lines]\n",
    "        return '\\n'.join(updated_lines)\n",
    "\n",
    "    def replace_if_empty(text, replacement='No relevant packages'):\n",
    "        if text is not None and len(text) == 0:\n",
    "            return replacement\n",
    "        return text\n",
    "\n",
    "    def maybe_start_on_next_line(string):\n",
    "        # If `string` is multiline, prepend a \\n to it.\n",
    "        if string is not None and len(string.split('\\n')) > 1:\n",
    "            return '\\n{}\\n'.format(string)\n",
    "        return string\n",
    "\n",
    "    mutable_dict = envinfo._asdict()\n",
    "\n",
    "    # If nvidia_gpu_models is multiline, start on the next line\n",
    "    mutable_dict['nvidia_gpu_models'] = \\\n",
    "        maybe_start_on_next_line(envinfo.nvidia_gpu_models)\n",
    "\n",
    "    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\n",
    "    dynamic_cuda_fields = [\n",
    "        'cuda_runtime_version',\n",
    "        'nvidia_gpu_models',\n",
    "        'nvidia_driver_version',\n",
    "    ]\n",
    "    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']\n",
    "    all_dynamic_cuda_fields_missing = all(\n",
    "        mutable_dict[field] is None for field in dynamic_cuda_fields)\n",
    "    if TORCH_AVAILABLE and not torch.cuda.is_available() and all_dynamic_cuda_fields_missing:\n",
    "        for field in all_cuda_fields:\n",
    "            mutable_dict[field] = 'No CUDA'\n",
    "        if envinfo.cuda_compiled_version is None:\n",
    "            mutable_dict['cuda_compiled_version'] = 'None'\n",
    "\n",
    "    # Replace True with Yes, False with No\n",
    "    mutable_dict = replace_bools(mutable_dict)\n",
    "\n",
    "    # Replace all None objects with 'Could not collect'\n",
    "    mutable_dict = replace_nones(mutable_dict)\n",
    "\n",
    "    # If either of these are '', replace with 'No relevant packages'\n",
    "    mutable_dict['pip_packages'] = replace_if_empty(mutable_dict['pip_packages'])\n",
    "    mutable_dict['conda_packages'] = replace_if_empty(mutable_dict['conda_packages'])\n",
    "\n",
    "    # Tag conda and pip packages with a prefix\n",
    "    # If they were previously None, they'll show up as ie '[conda] Could not collect'\n",
    "    if mutable_dict['pip_packages']:\n",
    "        mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n",
    "                                               '[{}] '.format(envinfo.pip_version))\n",
    "    if mutable_dict['conda_packages']:\n",
    "        mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n",
    "                                                 '[conda] ')\n",
    "    return env_info_fmt.format(**mutable_dict)\n",
    "\n",
    "\n",
    "def get_pretty_env_info():\n",
    "    return pretty_str(get_env_info())\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Collecting environment information...\")\n",
    "    output = get_pretty_env_info()\n",
    "    print(output)\n",
    "\n",
    "    if TORCH_AVAILABLE and hasattr(torch, 'utils') and hasattr(torch.utils, '_crash_handler'):\n",
    "        minidump_dir = torch.utils._crash_handler.DEFAULT_MINIDUMP_DIR\n",
    "        if sys.platform == \"linux\" and os.path.exists(minidump_dir):\n",
    "            dumps = [os.path.join(minidump_dir, dump) for dump in os.listdir(minidump_dir)]\n",
    "            latest = max(dumps, key=os.path.getctime)\n",
    "            ctime = os.path.getctime(latest)\n",
    "            creation_time = datetime.datetime.fromtimestamp(ctime).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n",
    "                  \"if this is related to your bug please include it when you file a report ***\"\n",
    "            print(msg, file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c0115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
